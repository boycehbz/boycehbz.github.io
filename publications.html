---
layout: page
title: Publications
---
<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HUANG-CVPR-2025.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.3rem">Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><b>Buzhen Huang</b>, Chen Li, Chongyang Xu, Dongyue Lu, Jinnan Chen, Yangang Wang, Gim Hee Lee</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="../works/CloseApp.html" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="papers/CVPR2025-CloseApp.pdf", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/CloseApp" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="PAN-CVPR-2025.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.2rem">Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</span>
        <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem">Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, <b>Buzhen Huang</b>, Bo Dai, Taku Komura, Jingbo Wang</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://liangpan99.github.io/TokenHSI/" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2503.19901", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/liangpan99/TokenHSI" target="_blank">[code]</a></span>
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HGM-ICLR-2025.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.5rem">Generalizable Human Gaussians from Single-View Image</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><i> International Conference on Learning Representations (<strong>ICLR</strong>), 2025</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem">Jinnan Chen, Chen Li, Jianfeng Zhang, Lingting Zhu, <b>Buzhen Huang</b>, Hanlin Chen, Gim Hee Lee</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://jinnan-chen.github.io/projects/HGM/" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2406.06050", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/jinnan-chen/HGM" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>


<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HUANG-CVPR-2024.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.3rem">Closely Interactive Human Reconstruction with Proxemics and Physics-Guided Adaption</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><b>Buzhen Huang</b>, Chen Li, Chongyang Xu, Liang Pan, Yangang Wang and Gim Hee Lee</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2404.11291", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.youtube.com/watch?v=hgE7ZDMFmhY&t=17s" target="_blank">[video]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/HumanInteraction" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="PL-3DV-2024.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.3rem">Synthesizing Physically Plausible Human Motions in 3D Scenes</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><i> International Conference on 3D Vision (<strong>3DV</strong>), 2024</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem">Liang Pan, Jingbo Wang, <b>Buzhen Huang</b>, Junyu Zhang, Haofan Wang, Xu Tang, Yangang Wang</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://liangpan99.github.io/InterScene/" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2308.09036", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/liangpan99/InterScene" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>


<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="grouprec-teaser.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.3rem">Reconstructing Groups of People with Hypergraph Relational Reasoning</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2023</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><b>Buzhen Huang</b>, Jingyi Ju, Zhihao Li and Yangang Wang</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.yangangwang.com/papers/iccv2023-grouprec/HUANG-GROUPREC-2023-07.html" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2308.15844", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/GroupRec#pseudo-dataset" target="_blank">[dataset]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/GroupRec" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="PGH-teaser.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.3rem">Physics-Guided Human Motion Capture with Pose Probability Modeling</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> International Joint Conferences on Artificial Intelligence (<strong>IJCAI</strong>), 2023</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem">Jingyi Ju*, <b>Buzhen Huang*</b>, Chen Zhu, Zhihao Li and Yangang Wang</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[project]</a></span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2308.09910", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/Me-Ditto/Physics-Guided-Mocap" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HBZ-TCSVT-2023.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.3rem">Simultaneously Recovering Multi-Person Meshes and Multi-View Cameras with Human Semantics</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><i> IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), 2023</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><b>Buzhen Huang</b>, Jingyi Ju, Yuan Shu and Yangang Wang</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span> -->
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.yangangwang.com/papers/iccv2023-grouprec/HUANG-GROUPREC-2023-07.html" target="_blank">[project]</a></span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://ieeexplore.ieee.org/document/10299685/", target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/DMMR" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HBZ-OOH-TPAMI2022.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:0.3rem">Object-Occluded Human Shape and Pose Estimation with Probabilistic Latent Consistency</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2022</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.5rem"><b>Buzhen Huang*</b>, Tianshu Zhang* and Yangang Wang</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.yangangwang.com/papers/ZHANG-OOH-2020-03.html" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://ieeexplore.ieee.org/document/9858644", target="_blank">[paper]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://gitee.com/seuvcl/CVPR2020-OOH" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HBZ-Pose2UV-2022-06.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:1.0rem">Pose2UV: Single-shot Multi-person Mesh Recovery with Deep UV Prior</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> IEEE Transactions on Image Processing (<strong>TIP</strong>), 2022</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.7rem"><b>Buzhen Huang*</b>, Tianshu Zhang* and Yangang Wang</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.yangangwang.com/papers/HBZ-Pose2UV-2022-06.html" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://ieeexplore.ieee.org/document/9817035" target="_blank">[paper]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.bilibili.com/video/BV19t4y1h7vJ" target="_blank">[video]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/3DMPB-dataset" target="_blank">[dataset]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/Pose2UV" target="_blank">[code]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HBZ-NM-2022-03.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:1.0rem">Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.7rem"><b>Buzhen Huang</b>, Liang Pan, Yuan Yang, Jingyi Ju and Yangang Wang</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*contribute equally)</span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.yangangwang.com/papers/HBZ-NM-2022-03.html" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2203.14065" target="_blank">[paper]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.bilibili.com/video/BV1W94y1f7ht" target="_blank">[video]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.bilibili.com/video/BV1pR4y1P7nJ?spm_id_from=333.337.search-card.all.click" target="_blank">[talk]</a></span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="" target="_blank">[sup.mat.]</a></span>  -->
    </div>
</div>
<hr style="margin-top:-40px;margin-bottom:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="HUANG-3DV-2021-10-teaser.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:1.5rem">Dynamic Multi-Person Mesh Recovery from Uncalibrated Multi-View Cameras</span>
        <!-- <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span> -->
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> International Conference on 3D Vision (<strong>3DV</strong>), 2021</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:1.1rem"><b>Buzhen Huang</b>, Yuan Shu, Tianshu Zhang and Yangang Wang</span>
        <!-- <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*contribute equally)</span> -->
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://arxiv.org/abs/2110.10355" target="_blank">[paper]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.bilibili.com/video/BV1Qq4y1d78S" target="_blank">[video]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="papers/3DV2021-SupplementaryMaterial.pdf" target="_blank">[supplementary]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/boycehbz/DMMR" target="_blank">[code]</a></span>
    </div>
</div>
<hr style="margin-top:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;margin-top: -20px" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left"; class="toleft"; width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="occlusion_teaser.jpg" >
    </div>
    <div align="left"; style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;">Object-Occluded Human Shape and Pose Estimation from a Single Color Image</span>
        <span style="display:inline-block;font-size:1.5rem;color:#FF1A1A;font-weight:bold;"> (Oral Presentation)</span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020</i></span>
        <span style="display:block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem">Tianshu Zhang*, <b>Buzhen Huang*</b>, Yangang Wang</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;padding-bottom:0.1rem">(*equal contribution)</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.yangangwang.com/papers/ZHANG-OOH-2020-03.html" target="_blank">[project]</a></span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Object-Occluded_Human_Shape_and_Pose_Estimation_From_a_Single_Color_CVPR_2020_paper.pdf" target="_blank">[paper]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://www.youtube.com/watch?v=8udm2OB0A-U&t=72s" target="_blank">[video]</a></span> <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://gitee.com/seuvcl/CVPR2020-OOH" target="_blank">[code]</a></span>
    </div>
</div>
<hr style="margin-top:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

<div width="100%" style="vertical-align: top;margin-top: -20px" align="center" style="width:15vmin;height:80px" class="side-side-side">
    <div align="left" class="toleft" width="30%">
        <img style="max-height: 200px; max-width: 200px;" width="100%" class="image" src="flower.jpg" >
    </div>
    <div align="left" style="vertical-align: top;" class="toright"; width="60%"; word-break:break-all;word-wrap:break-word;>
        <span style="display:inline-block;font-size:1.9rem;font-weight:bold;padding-bottom:1.5rem">A Flower Classification Framework Based on Ensemble of CNNs</span>
        <span style="display:inline-block;font-size:1.5rem;font-family:Arial;padding-bottom:0.1rem"><i> 19th Pacific-Rim Conference on Multimedia (<strong>PCM</strong>), 2018</i></span>
        <span style="display:inline-block;font-size:1.5rem;font-family:Arial;padding-bottom:1.1rem"><b>Buzhen Huang</b>, Youpeng Hu, Yaoqi Sun, Xinhong Hao, Chenggang Yan</span>
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://link.springer.com/chapter/10.1007/978-3-030-00764-5_22" target="_blank">[paper]</a></span> 
        <span style="display:inline-block;font-size:1.3rem;font-family:Arial;"><a href="https://github.com/yoooooohu/Flower-identification-by-ensemble-CNNs" target="_blank">[code]</a></span>
    </div>
</div>
<hr style="margin-top:-40px;height:0.1em;width:100%;font-size:0.1em;line-height:0.2em;border-top:1px solid #dedddd;"/>

