
<!-- saved from url=(0039)https://nileshkulkarni.github.io/nifty/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <!-- Google tag (gtag.js) -->
    <!-- <script async="" src="./NIFTY_files/js"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DB5J2MQV0D');
    </script> -->

    <!-- <script src="./NIFTY_files/jsapi" type="text/javascript"></script>
    <script type="text/javascript">
    google.load("jquery", "1.3.2");
    </script>     -->
<style type="text/css">
body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
}

h1 {
    font-weight: 300;
    margin: 0.4em;
}

/* p {
    margin: 0.2em;
} */

.disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
    padding: 20px;
}

video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

a:link,
a:visited {
    color: #1367a7;
    text-decoration: none;
}

a:hover {
    color: #208799;
}

td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
}

.layered-paper-big {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
    /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
}


.layered-paper {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35);
    /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
}

.vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
}

hr {
    margin: 0;
    border: 0;
    height: 1.5px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
}

.rotate {
    /* FF3.5+ */
    -moz-transform: rotate(-90.0deg);
    /* Opera 10.5 */
    -o-transform: rotate(-90.0deg);
    /* Saf3.1+, Chrome */
    -webkit-transform: rotate(-90.0deg);
    /* IE6,IE7 */
    filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);
    /* IE8 */
    -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083)";
    /* Standard */
    transform: rotate(-90.0deg);
}

c {
    white-space: nowrap;
    writing-mode: tb-rl;
    transform: rotate(-180.0deg);
}

    .topnav {
      background-color: #eeeeee;
      overflow: hidden;
    }

    .topnav div {
      max-width: 1070px;
      margin: 0 auto;
    }

    .topnav a {
      display: inline-block;
      color: black;
      text-align: center;
      vertical-align: middle;
      padding: 16px 16px;
      text-decoration: none;
      font-size: 16px;
    }

    .topnav img {
      width: 100%;
      margin: 0.2em 0px 0.3em 0px;
    }
    .authors div{
        text-align: center;
    }
    .content{
        margin-bottom: 2em;
        font-size: 12pt;
    }
    p {
        display: block;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0px;
        margin-inline-end: 0px;
    }


</style><title>CloseApp</title><meta property="og:title" content="nlos"></head>





    
    


<body>	

    <br>
    <center>
    <p>
        <span style="font-size:42px"> Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning </span>
    </p>
    </center>
    <br>
    <div align="center" class="authors">
       <a href="https://www.buzhenhuang.com/"> Buzhen Huang<sup>1,2</sup></a> 
       &nbsp;  
       <a href="https://chaneyddtt.github.io/"> Chen Li<sup>4,5</sup></a> 
       &nbsp;  
       <a href="https://github.com/Wil909"> Chongyang Xu<sup>3</sup></a> 
       &nbsp;  
       <a href="https://dylanorange.github.io/"> Dongyue Lu<sup>2</sup></a> 
       <br>
       <a href="https://jinnan-chen.github.io/"> Jinnan Chen<sup>2</sup></a> 
       &nbsp;  
       <a href="https://www.yangangwang.com/"> Yangang Wang<sup>1</sup></a> 
       &nbsp;  
       <a href="https://www.comp.nus.edu.sg/~leegh/"> Gim Hee Lee<sup>2</sup></a> 
      
    </div>
    <br>
    <!-- <div align=center class="authors">
        University of Michigan<sup>1</sup> 
        Google<sup>2</sup> 
        NVIDIA<sup>3</sup>
         

    </div> -->
    <div> 
        <table align="center" width="500px">
            <tbody><tr>
                <td style="text-align: center; vertical-align: middle;">
                    Southeast University<sup>1</sup>
                    National University of Singapore<sup>2</sup><br>
                    Sichuan University<sup>3</sup><br>
                    IHPC, Agency for Science, Technology and Research, Singapore<sup>4</sup><br>
                    CFAR, Agency for Science, Technology and Research, Singapore<sup>5</sup>
                </td>
            </tr>
        </tbody></table>
       
    </div>
    <div align="center" style="margin-top: 20px;">
        CVPR, 2025
    </div>
    
    <br>
    <table align="center" width="400px">
        <tbody><tr>
            <td align="center" width="50px">
                <center>
                    <a href="">[Paper]</a>
                </center>
            </td>
            <td align="center" width="50px">
                <center>
                    <span> <a href="">[Supp.] </a> </span> 
                </center>
            </td>
            <td align="center" width="50px">
                <center>
                    <span> <a href="">[GitHub] </a> </span> 
                </center>
            </td>
            <td align="center" width="50px">
                <center>
                    <span> <a href="">[Dataset] </a> </span> 
                </center>
            </td>
        </tr>
    </tbody></table>
    <br>
    <br>
    <!-- <hr> -->
    <table align="center" width="800px">
    <tbody><tr><td>
 
    <div align="center" class="content" width="400px">
    <img align="center" width="800px" src="CloseApp_files/teaser_1.jpg"> 
    <br>
    <p align="justify">
        Due to visual ambiguities and inter-person occlusions, existing human pose estimation methods cannot recover plausible close interactions from in-the-wild videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot accurately distinguish human semantics in such challenging scenarios. In this work, we find that human appearance can provide a straightforward cue to address these obstacles. Based on this observation, we propose a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts constrained by human appearances, social proxemics, and physical laws. Specifically, we first train a diffusion model to learn the human proxemic behavior and pose prior knowledge. The trained network and two optimizable tensors are then incorporated into a dual-branch optimization framework to reconstruct human motions and appearances. Several constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to assist the optimization. With the proxemics prior and diverse constraints, our method is capable of estimating accurate interactions from in-the-wild videos captured in complex environments. We further build a dataset with pseudo ground-truth interaction annotations, which may promote future research on pose estimation and human behavior understanding. Experimental results on several benchmarks demonstrate that our method outperforms existing approaches. The code and data will be publicly available for research purpose. 
    </p>
    </div>
    </td>  </tr>
    </tbody></table>
    
    <center>
        <h1>Visual Appearance</h1>
    </center>  
    <!-- <hr> -->
    <table align="center" width="800px">
        <tbody><tr>
        <td>
            <img align="center" width="800px" src="./CloseApp_files/appearance.jpg">
        </td>
        </tr>
        <tr>
        <td>
            <p>
                With predicted UV Gaussian maps, we can map the Gaussians to 3D space with a UV coordinate map and splat them to the image plane. We can then reason the depth ordinal relationship and image-model alignment with the rendered and original images.
            </p>
        </td>
        </tr>
    
    </tbody></table>
    
    <center>
        <h1>Method</h1>
    </center>  
    <hr>
    <table align="center" width="800px">
        <tbody><tr>
        <td>
            <img align="center" width="800px" src="./CloseApp_files/pipeline.jpg">
        </td>
        </tr>
        <tr>
        <td>
            <p>
                <strong>Overview of our framework.</strong> We propose a dual-branch optimization framework to reconstruct close human interactions from a monocular in-the-wild video. By optimizing the proxemics prior, U-Net backbone, and two optimizable tensors, the framework simultaneously predicts interactive motions and coarse appearances. With the constraints from 2D observations, physics, and prior knowledge, the framework can finally output 3D interactions with plausible body poses, natural proxemic relationships and accurate physical contacts.
            </p>
        </td>
        </tr>

    </tbody></table>
    <center>
        <h1>Qulaitative Comparisons</h1>
    </center>
    <table align="center" width="800px">
        <tbody><tr>
        <td colspan="2">
            <center>
            <video width="400" controls="" muted="" autoplay="" loop="">
                <source src="./CloseApp_files/sample1.mp4" type="video/mp4">
            </video>
            </center>
        </td>
        <td>
            <center>
                <video width="400" controls="" muted="" autoplay="" loop="">
                <source src="./CloseApp_files/sample2.mp4" type="video/mp4">
            </video>
            </center>
        </td> 
        </tr>
    </tbody></table>
    <table align="center" width="800px">
        <tbody>
        <tr>
        <td>
            <p>
                Our method leverages human appearance, proxemics, and physics to reduce visual ambiguity, resulting in improved performance.
            </p>
        </td>
        </tr>


    <!-- </tbody></table>
    <table align="center" width="800px">
        <tbody><tr>
            <td>
                <p>
                Given a final interaction pose of a person sitting on a chair / table we use a pre-trained motion model to 
                predict the past motion. Our generation follows a tree-like branching strategy and allows us to scalably create
                more data for an given interaction.
                </p>
            </td>
        </tr>
    </tbody></table>

    <center>
        <h1>Motion Generation Results with NIFTY</h1>
    </center>  
    <hr>
    <table align="center" width="800px">
        <tbody><tr>
            
            <td colspan="2">
                <center>
                <video width="400" controls="" muted="" autoplay="" loop="">
                    <source src="./assets/nifty_additional/medium_sit_cw.mp4" type="video/mp4">
                </video>
                </center>
            </td>   
            <td colspan="2">
                <center>
                <video width="400" controls="" muted="" autoplay="" loop="">
                    <source src="./assets/nifty_additional/medium_lift_stool.mp4" type="video/mp4">
                </video>
                </center>
            </td>     
        </tr>
        <tr>
        <td colspan="2">
            <p align="center">
            Sitting on a chair
            </p>
        </td>
        <td colspan="2">
            <p align="center">
            Lifting a stool
            </p>
        </td>
        </tr>
    </tbody></table>
    <table align="center" width="800px">
        <tbody><tr>
            <td>
                <a href="https://nileshkulkarni.github.io/nifty/supplementary.html#qual_results"><p align="center"> Additional results visit  this site</p> </a>
            </td>
        </tr>
    </tbody></table>
    <center>
        <h1>Qualitative Comparisons</h1>
    </center>  
    <hr>

    <table align="center" width="800px">
        <tbody><tr>
            <td>
                We conduct an user study to evaluate the quality of generated motions and compare to other baseline methods.
            </td>
        </tr>
        <tr>
            <td>
                <br>
            </td>
        </tr>
        <tr>
            <td align="center">
                <img width="400px" src="./NIFTY_files/user_study.png">
            </td>
        </tr>
        <tr>
            <td>
            We compare against two baselines a) Conditional VAE, and b) Conditional MDM. NIFTY's outputs are consistently 
            preferred over outputs from these methods. It is interesting to note that as compared to the 
            sythetic training data NIFTY is equally preferred.
            </td>
        </tr>
    </tbody></table>
    <table>
        <tbody><tr>
            <td colspan="4">
                <center>
                <video height="400" controls="" muted="" autoplay="" loop="">
                    <source src="./assets/baselines/medium_sit_baselines.mp4" type="video/mp4">
                </video>
                </center>
            </td>   
        </tr>
        <tr>
            <td align="center">
                <b> Sitting Interactions</b>
            </td>
        </tr>
    </tbody></table>
    <br><br>
    <table>
        <tbody><tr>
            <td colspan="4">
                <center>
                <video height="400" controls="" muted="" autoplay="" loop="">
                    <source src="./assets/baselines/medium_lift_baselines.mp4" type="video/mp4">
                </video>
                </center>
            </td>   
        </tr>
        <tr>
            <td align="center">
                <b>Lifting Interactions</b>
            </td>
        </tr>

    
    <br> -->

</tbody></table>
<center>
    <h1>Citation</h1>
</center>

<table align="center" width="800px">
    <tbody>
        <tr>
            <td>
                <div class="bibtex">
                    <pre>
@inproceedings{closeapp,
    title     = {Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning},
    author    = {Huang, Buzhen and Li, Chen and Xu, Chongyang and Lu, Dongyue and Chen, Jinnan and 
                Wang, Yangang and Lee, Gim Hee},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2025}}
                    </pre>
                </div>
            </td>
        </tr>
    </tbody>
</table>

<style>
.bibtex {
    font-family: "Courier New", monospace; /* 等宽字体 */
    white-space: pre-wrap;   /* 自动换行 */
    word-wrap: break-word;   /* 长文本折行 */
    text-align: left;        /* 左对齐 */
    font-size: 15px;         /* 增大字体 */
    max-width: 800px;            /* 限制最大宽度 */
    
}
h1 {
    font-family: "Times New Roman", serif;
    font-weight: bold;
    margin-bottom: 1px;
}
</style>


<!-- </tbody></table>
<center>
    <h1>Paper</h1>
</center>
<hr>
<br>
<table align="center" width="800px">
    <tbody><tr>
        <td colspan="2">
            <img class="layered-paper-big" width="150px" src="./CloseApp_files/paper.jpg">
        </td>
        <td colspan="2">
            <pre style="font-size: 15px; word-wrap: break-word; overflow-wrap: break-word;">
                @inproceedings{closeapp,
                    title     = {Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning},
                    author    = {Huang Buzhen and Li Chen and Xu Chongyang and Lu Dongyue and Chen Jinnan and Wang Yangang and Lee Gim Hee},
                    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                    year      = {2025}}
                </pre>
            
        </td>
    </tr> -->
    
    </tbody></table>
    <center>
        <h1>Acknowledgments</h1>
    </center>
    <hr>
    <table align="center" width="800px">
        <tbody>
        <tr>
        <td>
            <p>
                This research is supported by China Scholarship Council under Grant Number 202306090192. This project page template is based on <a href="https://research.nvidia.com/labs/toronto-ai/trace-pace/"> this page</a>.
            </p>
        </td>
        </tr>


    



</body></html>